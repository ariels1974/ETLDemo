# Zookeeper Service
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-service
  labels:
    app: zookeeper
spec:
  ports:
  - port: 2181
    name: client
  selector:
    app: zookeeper
---
# Zookeeper Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zookeeper
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
      - name: zookeeper
        image: confluentinc/cp-zookeeper:7.4.0
        ports:
        - containerPort: 2181
        env:
        - name: ZOOKEEPER_CLIENT_PORT
          value: "2181"
        - name: ZOOKEEPER_TICK_TIME
          value: "2000"
        - name: ZOOKEEPER_SYNC_LIMIT
          value: "2"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
---
# Kafka Internal Service
apiVersion: v1
kind: Service
metadata:
  name: kafka-service
  labels:
    app: kafka
spec:
  ports:
  - port: 9092
    name: kafka-port
  selector:
    app: kafka
  type: ClusterIP
---
# Kafka External Service (NodePort for external access)
apiVersion: v1
kind: Service
metadata:
  name: kafka-external
  labels:
    app: kafka
spec:
  ports:
  - port: 9093
    targetPort: 9093
    nodePort: 30092
    name: kafka-external
  selector:
    app: kafka
  type: NodePort
---
# Kafka Deployment optimized for large messages
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:7.4.0
        ports:
        - containerPort: 9092
        - containerPort: 9093
        env:
        # Basic Kafka configuration
        - name: KAFKA_BROKER_ID
          value: "1"
        - name: KAFKA_ZOOKEEPER_CONNECT
          value: "zookeeper-service:2181"
        - name: KAFKA_ADVERTISED_LISTENERS
          value: "PLAINTEXT://kafka-service:9092,PLAINTEXT_HOST://localhost:30092"
        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
          value: "PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT"
        - name: KAFKA_LISTENERS
          value: "PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:9093"
        - name: KAFKA_INTER_BROKER_LISTENER_NAME
          value: "PLAINTEXT"
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "1"
        - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
          value: "true"
        
        # Large message configuration - 50MB limits
        - name: KAFKA_MESSAGE_MAX_BYTES
          value: "52428800"  # 50MB broker-level limit
        - name: KAFKA_REPLICA_FETCH_MAX_BYTES
          value: "52428800"  # 50MB replica fetch limit
        - name: KAFKA_FETCH_MESSAGE_MAX_BYTES
          value: "52428800"  # 50MB fetch limit
        
        # Socket and buffer settings for large messages
        - name: KAFKA_SOCKET_SEND_BUFFER_BYTES
          value: "102400"    # 100KB
        - name: KAFKA_SOCKET_RECEIVE_BUFFER_BYTES
          value: "102400"    # 100KB
        - name: KAFKA_SOCKET_REQUEST_MAX_BYTES
          value: "104857600" # 100MB
        
        # Performance settings
        - name: KAFKA_NUM_NETWORK_THREADS
          value: "8"
        - name: KAFKA_NUM_IO_THREADS
          value: "8"
        - name: KAFKA_LOG_SEGMENT_BYTES
          value: "1073741824"  # 1GB log segment
        
        # Memory settings for JVM
        - name: KAFKA_HEAP_OPTS
          value: "-Xmx2G -Xms2G"
        
        resources:
          requests:
            memory: "2Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
---
# Job to create topics optimized for large messages
apiVersion: batch/v1
kind: Job
metadata:
  name: create-large-message-topics
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: kafka-topic-creator
        image: confluentinc/cp-kafka:7.4.0
        command: ["/bin/bash", "-c"]
        args:
        - |
          echo "Waiting for Kafka to be ready..."
          sleep 45
          
          # Create scraping-requests topic with large message support
          kafka-topics --create \
            --topic scraping-requests \
            --bootstrap-server kafka-service:9092 \
            --partitions 6 \
            --replication-factor 1 \
            --config max.message.bytes=52428800 \
            --config segment.bytes=1073741824 \
            --config retention.ms=604800000 \
            --if-not-exists
          echo "Topic 'scraping-requests' created with 50MB message limit"

          # Create scraping-data topic with large message support (main topic)
          kafka-topics --create \
            --topic scraping-data \
            --bootstrap-server kafka-service:9092 \
            --partitions 6 \
            --replication-factor 1 \
            --config max.message.bytes=52428800 \
            --config segment.bytes=1073741824 \
            --config retention.ms=604800000 \
            --config compression.type=snappy \
            --if-not-exists
          echo "Topic 'scraping-data' created with 50MB message limit and compression"

          # Create product-scraping-data topic with large message support
          kafka-topics --create \
            --topic product-scraping-data \
            --bootstrap-server kafka-service:9092 \
            --partitions 6 \
            --replication-factor 1 \
            --config max.message.bytes=52428800 \
            --config segment.bytes=1073741824 \
            --config retention.ms=604800000 \
            --config compression.type=snappy \
            --if-not-exists
          echo "Topic 'product-scraping-data' created with 50MB message limit and compression"
                   
          echo "Verifying topic configurations..."
          kafka-topics --describe --bootstrap-server kafka-service:9092
---
# Kafka UI for management and monitoring
apiVersion: v1
kind: Service
metadata:
  name: kafka-ui-service
  labels:
    app: kafka-ui
spec:
  ports:
  - port: 8080
    targetPort: 8080
    nodePort: 30080
    name: kafka-ui-port
  selector:
    app: kafka-ui
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-ui
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-ui
  template:
    metadata:
      labels:
        app: kafka-ui
    spec:
      containers:
      - name: kafka-ui
        image: provectuslabs/kafka-ui:latest
        ports:
        - containerPort: 8080
        env:
        - name: KAFKA_CLUSTERS_0_NAME
          value: "local"
        - name: KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS
          value: "kafka-service:9092"
        - name: KAFKA_CLUSTERS_0_ZOOKEEPER
          value: "zookeeper-service:2181"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
---
# MongoDB for storing large scraped data
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo:6.0
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_DATABASE
          value: ScrapingDb
        - name: MONGO_INITDB_ROOT_USERNAME
          value: root
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: password
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
# MongoDB Service
apiVersion: v1
kind: Service
metadata:
  name: mongodb-service
  labels:
    app: mongodb
spec:
  type: NodePort
  ports:
  - port: 27017
    targetPort: 27017
    nodePort: 32017
    name: mongo
  selector:
    app: mongodb
---
# Job to create unique index in MongoDB
apiVersion: batch/v1
kind: Job
metadata:
  name: mongodb-simple-index
spec:
  backoffLimit: 5
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: mongo-index-creator
        image: mongo:6.0
        command: ["/bin/bash", "-c"]
        args:
        - |
          echo "Testing MongoDB connection..."
          
          # Simple connection test
          for i in {1..30}; do
            if mongosh --host mongodb-service --port 27017 --username root --password password --authenticationDatabase admin --eval "db.adminCommand('ping')" --quiet; then
              echo "MongoDB connection successful!"
              break
            else
              echo "Attempt $i: MongoDB not ready, waiting..."
              sleep 10
            fi
          done
          
          # Create just the basic unique index first
          echo "Creating basic unique index..."
          mongosh --host mongodb-service --port 27017 --username root --password password --authenticationDatabase admin ScrapingDb --eval "
            db.ProductScrapingRecords.createIndex(
              { siteName: 1, description: 1, price: 1 },
              { unique: true, name: 'unique_product_index' }
            );
            print('Index created successfully');
            db.ProductScrapingRecords.getIndexes().forEach(idx => print('Index: ' + idx.name));
          "
          
          echo "Job completed!"
---
# InfluxDB for time-series data storage
apiVersion: v1
kind: Service
metadata:
  name: influxdb-service
  labels:
    app: influxdb
spec:
  ports:
    - port: 8086
      targetPort: 8086
      nodePort: 30086  
      name: influxdb
  selector:
    app: influxdb
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: influxdb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: influxdb
  template:
    metadata:
      labels:
        app: influxdb
    spec:
      containers:
        - name: influxdb
          image: influxdb:2.7
          ports:
            - containerPort: 8086
          env:
            - name: DOCKER_INFLUXDB_INIT_MODE
              value: "setup"
            - name: DOCKER_INFLUXDB_INIT_USERNAME
              value: "etluser"
            - name: DOCKER_INFLUXDB_INIT_PASSWORD
              value: "etlpassword"
            - name: DOCKER_INFLUXDB_INIT_ORG
              value: "etldemo"
            - name: DOCKER_INFLUXDB_INIT_BUCKET
              value: "etlstats"
            - name: DOCKER_INFLUXDB_INIT_ADMIN_TOKEN
              value: "etltoken"
          volumeMounts:
            - name: influxdb-storage
              mountPath: /var/lib/influxdb2
      volumes:
        - name: influxdb-storage
          persistentVolumeClaim:
            claimName: influxdb-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: influxdb-pvc
  labels:
    app: influxdb
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi